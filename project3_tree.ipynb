{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "deletable": false, "editable": false, "jupyter": {"outputs_hidden": true, "source_hidden": true}}, "outputs": [], "source": ["# Please do not change this cell because some hidden tests might depend on it.\n", "import os\n", "\n", "# Otter grader does not handle ! commands well, so we define and use our\n", "# own function to execute shell commands.\n", "def shell(commands, warn=True):\n", "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n", "     \n", "       Prints the result to stdout and returns the exit status. \n", "       Provides a printed warning on non-zero exit status unless `warn` \n", "       flag is unset.\n", "    \"\"\"\n", "    file = os.popen(commands)\n", "    print (file.read().rstrip('\\n'))\n", "    exit_status = file.close()\n", "    if warn and exit_status != None:\n", "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n", "    return exit_status\n", "\n", "shell(\"\"\"\n", "ls requirements.txt >/dev/null 2>&1\n", "if [ ! $? = 0 ]; then\n", " rm -rf .tmp\n", " git clone https://github.com/cs187-2020/project3.git .tmp\n", " mv .tmp/requirements.txt ./\n", " rm -rf .tmp\n", "fi\n", "pip install -q -r requirements.txt\n", "\"\"\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["# Initialize Otter\n", "import otter\n", "grader = otter.Notebook()"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "_6w_YiZWHiVi"}, "source": ["# Project 3: Parsing Using the Cocke-Kasami-Younger (CKY) Algorithm\n", "\n", "Constituency Parsing is an intermediary task in Natural Language Processing, where the goal is to extract the syntactic parse tree given a sentence.\n", "\n", "In this project, you will implement the CKY algorithm for context-free grammars (CFG). You will implement parsing algorithms for both a determinstic grammar (CFG) and a probabilistic grammar (PCFG).\n", "\n", "## Goals\n", "\n", "1. Finish a CFG for the ATIS dataset. \n", "2. Implement an algorithm for determining whether a parse exists or not given a sentence.\n", "3. Implement the CKY algorithm for parsing using CFGs.\n", "4. Construct a probabilistic context-free grammar (PCFG) based on a CFG.\n", "5. Implement the CKY algorithm for parsing using PCFGs."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "RVP1kKe1n3Wg"}, "source": ["## Setup"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "Kl16Hlvd01fL"}, "outputs": [], "source": ["!pip install nltk"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "l8sIw8-XFDDf"}, "outputs": [], "source": ["import nltk\n", "nltk.download('large_grammars')\n", "\n", "from nltk.grammar import ProbabilisticProduction, PCFG, Nonterminal\n", "from nltk.tree import Tree\n", "from nltk import treetransforms\n", "\n", "from collections import defaultdict, Counter"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "EaGmbg9vRnPt"}, "outputs": [], "source": ["# Grammar Coverage\n", "!wget -nv -N -P data https://raw.githubusercontent.com/nlp-course/data/master/ATIS/train.nl\n", "\n", "# PCFG Construction\n", "!wget -nv -N -P data https://raw.githubusercontent.com/nlp-course/data/master/ATIS/train.trees\n", "!wget -nv -N -P data https://raw.githubusercontent.com/nlp-course/data/master/ATIS/dev.trees\n", "!wget -nv -N -P data https://raw.githubusercontent.com/nlp-course/data/master/ATIS/test.trees\n", "\n", "# Tree Utils\n", "!wget -nv -N -P scripts https://raw.githubusercontent.com/nlp-course/data/master/scripts/trees/tree_utils.py\n", "!wget -nv -N -P scripts https://raw.githubusercontent.com/nlp-course/data/master/scripts/trees/evalb.py\n", "!wget -nv -N -P scripts https://raw.githubusercontent.com/nlp-course/data/master/scripts/trees/tree.py"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "jAwisi1cehil"}, "source": ["### Helper Functions\n", "\n", "The standard CKY algorithm requires the grammar to be in Chomsky normal form (CNF), i.e., only rules of the form *A -> B C*, *A -> a* are allowed where *A*, *B*, *C* are nonterminals and *a* is a terminal symbol. However, in some downstream applications (such as our next project) we want to use grammar rules of more general forms, such as *A -> B C D*. To satisfy both of these constraints, we will convert the grammar to CNF, parse using CKY, and then convert back to the form of the original grammar. We have provided helper functions to perform those conversions.\n", "\n", "To convert a grammar to CNF:\n", "\n", "`cnf_grammar, cnf_grammar_wunaries = get_cnf_grammar(grammar)`\n", "\n", "To convert a tree output from CKY back to the original form of the grammar in place:\n", "\n", "`un_cnf(tree, cnf_grammar_wunaries)`\n", "\n", "Note that above we need to pass in `cnf_grammar_wunaries`, an intermediate version of the grammar before removing unary rules. It's the second returned value of `get_cnf_grammar`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "xoFFMDdsxODo"}, "outputs": [], "source": ["from scripts.tree_utils import get_cnf_grammar, un_cnf"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "NwhiYW0SFDDp"}, "source": ["## A custom ATIS grammar"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "DP-62dlTFDDp"}, "source": ["To parse, we need a grammar. In this project, we will use a hand-crafted grammar for the ATIS dataset. It captures a large fraction of the language in the ATIS dataset while including a minimal number of rules. This tiny grammar will be used again in the next project for a question answering application.\n"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "D_JNkHfm2qjl"}, "source": ["### Goal 1: Finish the CFG for the ATIS dataset\n", "\n", "Fill in the missing rule for `PREJ`. This is a production rule that captures all the \"junk\" before the actual semantically meaningful question. For example, in \"i would like a flight between boston and dallas\", \"i would like\" carries no meaningful information hence is considered \"junk\". \n", "\n", "*Hint: you will make use of the `JUNK` preduction*"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "37s5jbAu2kk7"}, "outputs": [], "source": ["#TODO\n", "miniATIS = nltk.CFG.fromstring(\"\"\"\n", "S -> PREJ DET ADJS FLIGHT PPS | DET ADJS FLIGHT PPS | PREJ ADJS FLIGHT PPS |  ADJS FLIGHT PPS | PREJ DET FLIGHT PPS | DET FLIGHT PPS | PREJ FLIGHT PPS | FLIGHT PPS | PREJ DET ADJS FLIGHT  | DET ADJS FLIGHT | PREJ ADJS FLIGHT | ADJS FLIGHT | PREJ DET FLIGHT | DET FLIGHT | PREJ FLIGHT | FLIGHT \n", "PPS -> PP | PP PPS\n", "ADJS -> ADJ | ADJ ADJS\n", "\n", "FLIGHT -> 'flights' | 'flight' | 'to' 'fly'\n", "\n", "PREJ -> TODO<<add<<here<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n", "JUNK -> 'me' | 'show' | 'now' | 'only' | 'can' | 'you' | 'the' | 'itinerary' | 'of' | 'also' | 'a' | 'list' | 'could' | 'give' | 'which' | 'what' | 'is' | \"what's\" | 'are' | 'my' | 'choices' | 'for' | 'i' | 'would' | 'like' | \"i'd\" | 'to' | 'see' | 'have' | 'make' | 'book' | 'find' | 'information' | 'on' | 'know' | 'some' | 'hello' | 'yes' | 'please' | 'repeat' | 'do' | 'have' | 'there' | 'need' | 'hi' | 'get' | 'may' | 'listing' | 'listings' | 'travel' | 'arrangements' | 'okay' | 'want' | 'tell' | 'about' | 'how' | 'would' | 'be' | 'able' | 'put' | 'requesting' | \"i'm\" | 'looking' | 'display' | UNK\n", "\n", "ADJ -> AIRLINE | SIMPLEWEEKDAY | FLIGHTTYPE | SIMPLETIME | \"monday's\" | \"tuesday's\" | \"wednesday's\" | \"thursday's\" | \"friday's\" | \"saturday's\" | \"sunday's\" | 'available' | 'possible' | 'first' 'class' | 'economy' | 'thrift' 'economy' | 'cheapest' | 'lowest' 'cost' | 'least' 'expensive' | 'most' 'expensive' | 'weekday' | 'daily' | 'last' | 'first' | 'dinner' | 'transcontinental'  \n", "\n", "DET -> 'all' 'the' | 'all' | A | 'an' | THE | 'any' | 'all' 'of' 'the'\n", "BETWEEN -> 'between' \n", "AND -> 'and'\n", "OR -> 'or'\n", "EITHER -> 'either'\n", "OF -> 'of'\n", "THE -> 'the'\n", "A -> 'a'\n", "\n", "PP -> PPLACE PLACE OR PLACE | PPLACE EITHER PLACE OR PLACE | PPLACE PLACE | EITHER PLACE OR PLACE | PLACE OR PLACE | PLACE | BETWEEN PLACE AND PLACE | BETWEEN TIME AND TIME | PTIME TIME | TIME | PDAY WEEKDAY | WEEKDAY TIME | WEEKDAY | PDAY WEEKDAY TIME | PDAY DATE | DATE | PAIRLINE AIRLINE | AIRCRAFT | FLIGHTTYPE | FARETYPE | PRICE | FOOD | AVAIL | POSTJ\n", "\n", "PPLACE -> 'to' | 'that' 'arrive' 'at' | 'that' 'arrives' 'in' | 'coming' 'back' 'to' | 'that' 'go' 'to' | 'and' 'then' 'to' | 'arriving' 'in' | 'and' 'arriving' 'in' | 'and' 'arrive' 'in' | 'to' 'arrive' 'in' | 'arrive' 'in' | 'going' 'to' | 'into' | 'for' | 'with' 'the' 'destination' 'city' 'of' | 'arriving' | 'goes' 'to' | 'flying' 'into' | 'goes' 'on' 'to' | 'reaching' | 'in' | 'and' 'then' | 'arriving' 'to' | 'from' | 'leaving' | 'return' 'from' | 'leaving' 'from' | 'departing' 'from' | 'departing' | 'go' 'from' | 'going' 'from' | 'back' 'from' | 'that' 'goes' 'from' | 'that' 'departs' | 'which' 'leaves' 'from' | 'which' 'leave' | 'that' 'leave' | 'originating' 'in' | 'leave' | 'out' 'of' | 'leaves' 'from' | 'to' 'get' 'from' | 'via' | 'with' 'a' 'stopover' 'in' | 'with' 'a' 'layover' 'in' | 'with' 'a' 'stopover' 'at' | 'and' 'a' 'stopover' 'in' | 'stop' 'in' | 'stopping' 'in' | 'make' 'a' 'stop' 'in' | 'with' 'a' 'stop' 'in' | 'with' 'one' 'stop' 'in' | 'go' 'through' | 'which' 'go' 'through' | 'makes' 'a' 'stopover' 'in' | 'that' 'stops' 'in' | 'that' 'stops' 'over' 'in' | 'by' 'way' 'of' | 'connecting' 'through' | 'that' 'will' 'stop' 'in' | 'which' 'connects' 'in' | 'arriving' 'and' 'departing' 'at'\n", "\n", "PLACE -> 'anywhere' | 'atlanta' | 'austin' | 'baltimore' | 'boston' | 'boston' 'logan' | 'burbank' | 'bwi' | 'charlotte' | 'chicago' | 'cincinnati' | 'cleveland' | 'cleveland' 'ohio' | 'columbus' | 'dallas' 'fort' 'worth' | 'dallas' | 'denver' | 'denver' 'colorado' | 'detroit' | 'fort' 'worth' | 'general' 'mitchell' 'international' | 'general' 'mitchell' | 'houston' | 'indianapolis' | 'jfk' | 'kansas' 'city' | 'laguardia' 'airport' | 'las' 'vegas' | 'long' 'beach' | 'los' 'angeles' | 'love' 'field' | 'memphis' | 'miami' | 'milwaukee' | 'minneapolis' | 'montreal' | 'montreal' 'quebec' | 'montreal' 'canada' | 'nashville' | 'new' 'york' 'city' | 'new' 'york' | 'newark' | 'newark' 'new' 'jersey' | 'oakland' | 'oakland' 'california' | 'ontario' | 'orlando' | 'orlando' 'florida' | 'philadelphia' | 'philly' | 'phoenix' | 'pittsburgh' | 'salt' 'lake' 'city' | 'san' 'diego' | 'san' 'diego' 'california' | 'san' 'francisco' | 'san' 'jose' | 'seattle' | 'st.' 'louis' | 'st.' 'paul' | 'st.' 'petersburg' | 'tacoma' | 'tacoma' 'washington' | 'tampa' | 'toronto' | 'various' 'cities' | 'washington' | 'washington' 'dc' | 'dc' | 'westchester' 'county' | UNK\n", "\n", "PTIME -> 'that' 'arrive' 'before' | 'that' 'arrives' 'before' | 'arriving' 'before' | 'arrival' 'by' | 'arrives' | 'before' | 'departing' 'before' | 'that' 'leaves' 'before' | 'which' 'arrive' 'before' | 'by' | 'around' | 'that' 'return' 'around' | 'that' 'gets' 'in' 'around' | 'at' | 'arriving' 'around' | 'arriving' 'about' | 'that' 'arrive' 'soon' 'after' | 'leaving' 'at' | 'leaving' | 'which' 'leave' 'after' | 'leaving' 'after' | 'after' | 'departing' 'after' | 'that' 'depart' 'after' | 'departing' 'at' | 'arriving' 'after' | 'in' | 'departing' 'in' | 'on' | 'that' 'leaves' 'in'\n", "\n", "TIME -> SIMPLETIME | 'the' 'afternoon' | 'the' 'late' 'afternoon' | 'the' 'evening' | 'the' 'morning' | 'the' 'early' 'am' | 'the' 'day' | 'mornings' | 'afternoons' | 'evenings' | 'as' 'early' 'as' 'possible' | 'earliest' 'possible' 'time' | 'as' 'soon' 'thereafter' 'as' 'possible' | UNK\n", "\n", "SIMPLETIME -> 'one' | 'two' | 'three' | 'four' | 'five' | 'six' | 'seven' | 'eight' | 'nine' | 'ten' | 'eleven' | 'twelve' | '1' | '2' | '3' |'4' | '5' | '6' | '7' |  '8' | '9' | '10' | '11' | '12' | '1pm' | '2pm' | '3pm' | '4pm' | '5pm' | '6pm' | '7pm' | '8pm' | '9pm' | '10pm' | '11pm' | '12pm' | '1am' | '2am' | '3am' | '4am' | '5am' | '6am' | '7am' | '8am' | '9am' | '10am' | '11am' | '12am' | '230' | '1505' | '630am' | '720am' | '723pm' | '819' | '845' | '1026' | '1145am' | '1' \"o'clock\" | '1' \"o'clock\" 'am' | \"1\" \"o'clock\" \"pm\" | \"2\" \"o'clock\" | \"2\" \"o'clock\" \"am\" | \"2\" \"o'clock\" \"pm\" | \"3\" \"o'clock\" | \"3\" \"o'clock\" \"am\" | \"3\" \"o'clock\" \"pm\" | \"4\" \"o'clock\" | \"4\" \"o'clock\" \"pm\" | \"4\" \"o'clock\" \"am\" | \"5\" \"o'clock\" | \"5\" \"o'clock\" \"am\" | \"5\" \"o'clock\" \"pm\" | \"6\" \"o'clock\" | \"6\" \"o'clock\" \"pm\" | \"6\" \"o'clock\" \"am\" | \"7\" \"o'clock\" | \"7\" \"o'clock\" \"pm\" | \"7\" \"o'clock\" \"am\" | \"8\" \"o'clock\" | \"8\" \"o'clock\" \"am\" | \"8\" \"o'clock\" \"pm\" | \"9\" \"o'clock\" | \"9\" \"o'clock\" \"pm\" | \"9\" \"o'clock\" \"am\" | \"10\" \"o'clock\" | \"10\" \"o'clock\" \"am\" | \"10\" \"o'clock\" \"pm\" | \"11\" \"o'clock\" | \"11\" \"o'clock\" \"am\" | \"11\" \"o'clock\" \"pm\" | \"12\" \"o'clock\" | \"12\" \"o'clock\" \"pm\" | \"12\" \"o'clock\" \"am\" |  'noon' | '12' 'noon' | \"12\" \"o'clock\" \"noon\" | 'midnight' | '12' 'midnight' | 'lunch' 'time' | 'dinnertime' |  'evening' | 'night' | 'morning' | 'afternoon' | 'early' | 'late' | 'tonight' | 'earliest' 'possible' | 'earliest' | 'latest' 'possible' | 'latest' \n", "\n", "\n", "PDAY -> 'on' | 'returning' 'on' | 'of' | 'for' | 'next' | 'the' 'next' | 'in' 'the' 'next' | 'of' 'next' | 'leaving' | 'arriving' | 'arriving' 'on' | 'which' 'leave' | 'that' 'arrive' 'on' | 'leaving' 'on' | 'which' 'arrive' 'on' | 'a' 'week' 'from'\n", "\n", "SIMPLEWEEKDAY -> 'saturday' | 'sunday' | 'monday' | 'tuesday' | 'wednesday' | 'thursday' | 'friday' \n", "WEEKDAY -> SIMPLEWEEKDAY | A SIMPLEWEEKDAY | 'saturdays' | 'sundays' | 'mondays' | 'tuesdays' | 'wednesdays' | 'thursdays' | 'fridays' | 'this' 'saturday' | 'this' 'sunday' | 'this' 'monday' | 'this' 'tuesday' | 'this' 'wednesday' | 'this' 'thursday' | 'this' 'friday' | 'this' 'coming' 'saturday' | 'this' 'coming' 'sunday' | 'this' 'coming' 'monday' | 'this' 'coming' 'tuesday' | 'this' 'coming' 'wednesday' | 'this' 'coming' 'thursday' | 'this' 'coming' 'friday' | 'day' | 'week' | 'today' | 'tomorrow' | 'the' 'day' 'after' 'tomorrow' | 'a' 'weekday' | 'weekdays'\n", "\n", "DATE -> MONTH DAY YEAR | MONTH DAY | THE DAY | MONTH DAY OR DAY | THE DAY OR DAY | EITHER MONTH DAY OR DAY | EITHER THE DAY OR THE DAY | THE DAY OF MONTH | DAY OF MONTH | EITHER THE DAY OR THE DAY OF MONTH | THE DAY OF MONTH OR THE DAY OF MONTH | EITHER THE DAY OF MONTH OR THE DAY OF MONTH\n", "MONTH -> 'january' | 'february' | 'march' | 'april' | 'may' | 'june' | 'july' | 'august' | 'september' | 'october' | 'november' | 'december'\n", "DAY -> 'first' | 'second' | 'third' | 'fourth' | 'fifth' | 'sixth' | 'seventh' | 'eighth' | 'ninth' | 'tenth' | 'eleventh' | 'twelfth' | 'thirteenth' | 'fourteenth' | 'fifteenth' | 'sixteenth' | 'seventeenth' | 'eighteenth' | 'nineteenth' | 'twentieth' | 'twenty-first' | 'twenty' 'first' | 'twenty-second' | 'twenty' 'second' | 'twenty-third' | 'twenty' 'third' | 'twenty-fourth' | 'twenty' 'fourth' | 'twenty-fifth' | 'twenty' 'fifth' | 'twenty-sixth' | 'twenty' 'sixth' | 'twenty-seventh' | 'twenty' 'seventh' | 'twenty-eighth' | 'twenty' 'eighth' | 'twenty-ninth' | 'twenty' 'ninth' | 'thirtieth' | 'thirty-first' | 'thirty' 'first' | 'one' | 'two' | 'three' | 'four' | 'five' | 'six' | 'seven' | 'eight' | 'nine' | 'ten' | 'eleven' | 'twelve' | 'thirteen' | 'fourteen' | 'fifteen' | 'sixteen' | 'seventeen' | 'eighteen' | 'nineteen' | 'twenty' | 'twenty' 'one' | 'twenty' 'two' | 'twenty' 'three' | 'twenty' 'four' | 'twenty' 'five' | 'twenty' 'six' | 'twenty' 'seven' | 'twenty' 'eight' | 'twenty' 'nine' | 'thirty' | 'thirty' 'one'\n", "YEAR -> '1991' | '1992'\n", "\n", "PAIRLINE -> 'on' | 'using' | 'of' | 'with'\n", "\n", "AIRLINE -> 'continental' | 'continental' 'airline' | 'continental' 'airlines' | 'american' 'airlines' | 'american' 'airlines' | 'american' | 'united' 'airlines' | 'united' | 'united' 'airline' | 'northwest' 'and' 'united' | 'northwest' 'airlines' | 'northwest' 'airline' | 'northwest' | 'us' 'air' | 'us' 'airlines' | 'delta' | 'twa' | 'air' 'canada' | 'eastern' 'airlines' | 'midwest' 'express' | 'trans' 'world' 'airline'\n", "\n", "AIRCRAFT -> 'using' 'a' 'j31' 'aircraft'\n", "\n", "FLIGHTTYPE -> 'round' 'trip' | 'round-trip' | 'nonstop' | 'one' 'way' | 'return' | 'direct' | 'connecting' | 'direct' 'and' 'connecting' | 'nonstop' 'or' 'connecting'\n", "\n", "FARETYPE -> 'with' 'economy' 'fares' | 'qualify' 'for' 'fare' 'code' 'qx' | 'with' 'first' 'class' 'service' | 'that' 'offer' 'first' 'class' | 'on' 'first' 'class' | 'economy' 'class' | 'with' 'the' 'lowest' 'one' 'way' 'fares' | 'with' 'a' 'class' 'of' 'service' 'code' 'f' | 'with' 'q' 'fares' | 'first' 'class' 'fare'\n", "\n", "PRICE -> 'less' 'than' '1100' 'dollars' | 'with' 'the' 'highest' 'fare' | 'the' 'cheapest' 'way' 'possible' | 'less' 'than' '466' 'dollars' | 'lowest' 'cost' | 'least' 'expensive' | 'most' 'expensive' | 'cheapest'\n", "\n", "FOOD -> 'dinner' | 'a' 'meal' | 'lunch' | 'breakfast'\n", "\n", "AVAIL -> 'available' | 'i' 'can' 'get' \n", "\n", "POSTJ -> 'please' | 'there' | 'are' | 'currently' | 'do' | 'you' | 'have' | 'fares' | 'information' | 'i' | 'want' | 'would' | 'like' | 'the' | 'flight' | 'be' | 'go' | 'departures' | 'is' | 'such' | 'a' | 'that' | 'serves' | 'both' | 'and' | 'along' | 'with' | 'can' | 'get' | \"i'd\" | 'traveling' | 'for' | 'me' | UNK | '.' | '?'\n", "\"\"\")"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "dKay90BEEI4R"}, "source": ["#### Solution<!--Solution-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "jzLq3rpAFDDq"}, "outputs": [], "source": ["#Solution\n", "miniATIS = nltk.CFG.fromstring(\"\"\"\n", "S -> PREJ DET ADJS FLIGHT PPS | DET ADJS FLIGHT PPS | PREJ ADJS FLIGHT PPS |  ADJS FLIGHT PPS | PREJ DET FLIGHT PPS | DET FLIGHT PPS | PREJ FLIGHT PPS | FLIGHT PPS | PREJ DET ADJS FLIGHT  | DET ADJS FLIGHT | PREJ ADJS FLIGHT | ADJS FLIGHT | PREJ DET FLIGHT | DET FLIGHT | PREJ FLIGHT | FLIGHT \n", "PPS -> PP | PP PPS\n", "ADJS -> ADJ | ADJ ADJS\n", "\n", "FLIGHT -> 'flights' | 'flight' | 'to' 'fly'\n", "\n", "PREJ -> JUNK PREJ | JUNK\n", "JUNK -> 'me' | 'show' | 'now' | 'only' | 'can' | 'you' | 'the' | 'itinerary' | 'of' | 'also' | 'a' | 'list' | 'could' | 'give' | 'which' | 'what' | 'is' | \"what's\" | 'are' | 'my' | 'choices' | 'for' | 'i' | 'would' | 'like' | \"i'd\" | 'to' | 'see' | 'have' | 'make' | 'book' | 'find' | 'information' | 'on' | 'know' | 'some' | 'hello' | 'yes' | 'please' | 'repeat' | 'do' | 'have' | 'there' | 'need' | 'hi' | 'get' | 'may' | 'listing' | 'listings' | 'travel' | 'arrangements' | 'okay' | 'want' | 'tell' | 'about' | 'how' | 'would' | 'be' | 'able' | 'put' | 'requesting' | \"i'm\" | 'looking' | 'display' | UNK\n", "\n", "ADJ -> AIRLINE | SIMPLEWEEKDAY | FLIGHTTYPE | SIMPLETIME | \"monday's\" | \"tuesday's\" | \"wednesday's\" | \"thursday's\" | \"friday's\" | \"saturday's\" | \"sunday's\" | 'available' | 'possible' | 'first' 'class' | 'economy' | 'thrift' 'economy' | 'cheapest' | 'lowest' 'cost' | 'least' 'expensive' | 'most' 'expensive' | 'weekday' | 'daily' | 'last' | 'first' | 'dinner' | 'transcontinental'  \n", "\n", "DET -> 'all' 'the' | 'all' | A | 'an' | THE | 'any' | 'all' 'of' 'the'\n", "BETWEEN -> 'between' \n", "AND -> 'and'\n", "OR -> 'or'\n", "EITHER -> 'either'\n", "OF -> 'of'\n", "THE -> 'the'\n", "A -> 'a'\n", "\n", "PP -> PPLACE PLACE OR PLACE | PPLACE EITHER PLACE OR PLACE | PPLACE PLACE | EITHER PLACE OR PLACE | PLACE OR PLACE | PLACE | BETWEEN PLACE AND PLACE | BETWEEN TIME AND TIME | PTIME TIME | TIME | PDAY WEEKDAY | WEEKDAY TIME | WEEKDAY | PDAY WEEKDAY TIME | PDAY DATE | DATE | PAIRLINE AIRLINE | AIRCRAFT | FLIGHTTYPE | FARETYPE | PRICE | FOOD | AVAIL | POSTJ\n", "\n", "PPLACE -> 'to' | 'that' 'arrive' 'at' | 'that' 'arrives' 'in' | 'coming' 'back' 'to' | 'that' 'go' 'to' | 'and' 'then' 'to' | 'arriving' 'in' | 'and' 'arriving' 'in' | 'and' 'arrive' 'in' | 'to' 'arrive' 'in' | 'arrive' 'in' | 'going' 'to' | 'into' | 'for' | 'with' 'the' 'destination' 'city' 'of' | 'arriving' | 'goes' 'to' | 'flying' 'into' | 'goes' 'on' 'to' | 'reaching' | 'in' | 'and' 'then' | 'arriving' 'to' | 'from' | 'leaving' | 'return' 'from' | 'leaving' 'from' | 'departing' 'from' | 'departing' | 'go' 'from' | 'going' 'from' | 'back' 'from' | 'that' 'goes' 'from' | 'that' 'departs' | 'which' 'leaves' 'from' | 'which' 'leave' | 'that' 'leave' | 'originating' 'in' | 'leave' | 'out' 'of' | 'leaves' 'from' | 'to' 'get' 'from' | 'via' | 'with' 'a' 'stopover' 'in' | 'with' 'a' 'layover' 'in' | 'with' 'a' 'stopover' 'at' | 'and' 'a' 'stopover' 'in' | 'stop' 'in' | 'stopping' 'in' | 'make' 'a' 'stop' 'in' | 'with' 'a' 'stop' 'in' | 'with' 'one' 'stop' 'in' | 'go' 'through' | 'which' 'go' 'through' | 'makes' 'a' 'stopover' 'in' | 'that' 'stops' 'in' | 'that' 'stops' 'over' 'in' | 'by' 'way' 'of' | 'connecting' 'through' | 'that' 'will' 'stop' 'in' | 'which' 'connects' 'in' | 'arriving' 'and' 'departing' 'at'\n", "\n", "PLACE -> 'anywhere' | 'atlanta' | 'austin' | 'baltimore' | 'boston' | 'boston' 'logan' | 'burbank' | 'bwi' | 'charlotte' | 'chicago' | 'cincinnati' | 'cleveland' | 'cleveland' 'ohio' | 'columbus' | 'dallas' 'fort' 'worth' | 'dallas' | 'denver' | 'denver' 'colorado' | 'detroit' | 'fort' 'worth' | 'general' 'mitchell' 'international' | 'general' 'mitchell' | 'houston' | 'indianapolis' | 'jfk' | 'kansas' 'city' | 'laguardia' 'airport' | 'las' 'vegas' | 'long' 'beach' | 'los' 'angeles' | 'love' 'field' | 'memphis' | 'miami' | 'milwaukee' | 'minneapolis' | 'montreal' | 'montreal' 'quebec' | 'montreal' 'canada' | 'nashville' | 'new' 'york' 'city' | 'new' 'york' | 'newark' | 'newark' 'new' 'jersey' | 'oakland' | 'oakland' 'california' | 'ontario' | 'orlando' | 'orlando' 'florida' | 'philadelphia' | 'philly' | 'phoenix' | 'pittsburgh' | 'salt' 'lake' 'city' | 'san' 'diego' | 'san' 'diego' 'california' | 'san' 'francisco' | 'san' 'jose' | 'seattle' | 'st.' 'louis' | 'st.' 'paul' | 'st.' 'petersburg' | 'tacoma' | 'tacoma' 'washington' | 'tampa' | 'toronto' | 'various' 'cities' | 'washington' | 'washington' 'dc' | 'dc' | 'westchester' 'county' | UNK\n", "\n", "PTIME -> 'that' 'arrive' 'before' | 'that' 'arrives' 'before' | 'arriving' 'before' | 'arrival' 'by' | 'arrives' | 'before' | 'departing' 'before' | 'that' 'leaves' 'before' | 'which' 'arrive' 'before' | 'by' | 'around' | 'that' 'return' 'around' | 'that' 'gets' 'in' 'around' | 'at' | 'arriving' 'around' | 'arriving' 'about' | 'that' 'arrive' 'soon' 'after' | 'leaving' 'at' | 'leaving' | 'which' 'leave' 'after' | 'leaving' 'after' | 'after' | 'departing' 'after' | 'that' 'depart' 'after' | 'departing' 'at' | 'arriving' 'after' | 'in' | 'departing' 'in' | 'on' | 'that' 'leaves' 'in'\n", "\n", "TIME -> SIMPLETIME | 'the' 'afternoon' | 'the' 'late' 'afternoon' | 'the' 'evening' | 'the' 'morning' | 'the' 'early' 'am' | 'the' 'day' | 'mornings' | 'afternoons' | 'evenings' | 'as' 'early' 'as' 'possible' | 'earliest' 'possible' 'time' | 'as' 'soon' 'thereafter' 'as' 'possible' | UNK\n", "\n", "SIMPLETIME -> 'one' | 'two' | 'three' | 'four' | 'five' | 'six' | 'seven' | 'eight' | 'nine' | 'ten' | 'eleven' | 'twelve' | '1' | '2' | '3' |'4' | '5' | '6' | '7' |  '8' | '9' | '10' | '11' | '12' | '1pm' | '2pm' | '3pm' | '4pm' | '5pm' | '6pm' | '7pm' | '8pm' | '9pm' | '10pm' | '11pm' | '12pm' | '1am' | '2am' | '3am' | '4am' | '5am' | '6am' | '7am' | '8am' | '9am' | '10am' | '11am' | '12am' | '230' | '1505' | '630am' | '720am' | '723pm' | '819' | '845' | '1026' | '1145am' | '1' \"o'clock\" | '1' \"o'clock\" 'am' | \"1\" \"o'clock\" \"pm\" | \"2\" \"o'clock\" | \"2\" \"o'clock\" \"am\" | \"2\" \"o'clock\" \"pm\" | \"3\" \"o'clock\" | \"3\" \"o'clock\" \"am\" | \"3\" \"o'clock\" \"pm\" | \"4\" \"o'clock\" | \"4\" \"o'clock\" \"pm\" | \"4\" \"o'clock\" \"am\" | \"5\" \"o'clock\" | \"5\" \"o'clock\" \"am\" | \"5\" \"o'clock\" \"pm\" | \"6\" \"o'clock\" | \"6\" \"o'clock\" \"pm\" | \"6\" \"o'clock\" \"am\" | \"7\" \"o'clock\" | \"7\" \"o'clock\" \"pm\" | \"7\" \"o'clock\" \"am\" | \"8\" \"o'clock\" | \"8\" \"o'clock\" \"am\" | \"8\" \"o'clock\" \"pm\" | \"9\" \"o'clock\" | \"9\" \"o'clock\" \"pm\" | \"9\" \"o'clock\" \"am\" | \"10\" \"o'clock\" | \"10\" \"o'clock\" \"am\" | \"10\" \"o'clock\" \"pm\" | \"11\" \"o'clock\" | \"11\" \"o'clock\" \"am\" | \"11\" \"o'clock\" \"pm\" | \"12\" \"o'clock\" | \"12\" \"o'clock\" \"pm\" | \"12\" \"o'clock\" \"am\" |  'noon' | '12' 'noon' | \"12\" \"o'clock\" \"noon\" | 'midnight' | '12' 'midnight' | 'lunch' 'time' | 'dinnertime' |  'evening' | 'night' | 'morning' | 'afternoon' | 'early' | 'late' | 'tonight' | 'earliest' 'possible' | 'earliest' | 'latest' 'possible' | 'latest' \n", "\n", "\n", "PDAY -> 'on' | 'returning' 'on' | 'of' | 'for' | 'next' | 'the' 'next' | 'in' 'the' 'next' | 'of' 'next' | 'leaving' | 'arriving' | 'arriving' 'on' | 'which' 'leave' | 'that' 'arrive' 'on' | 'leaving' 'on' | 'which' 'arrive' 'on' | 'a' 'week' 'from'\n", "\n", "SIMPLEWEEKDAY -> 'saturday' | 'sunday' | 'monday' | 'tuesday' | 'wednesday' | 'thursday' | 'friday' \n", "WEEKDAY -> SIMPLEWEEKDAY | A SIMPLEWEEKDAY | 'saturdays' | 'sundays' | 'mondays' | 'tuesdays' | 'wednesdays' | 'thursdays' | 'fridays' | 'this' 'saturday' | 'this' 'sunday' | 'this' 'monday' | 'this' 'tuesday' | 'this' 'wednesday' | 'this' 'thursday' | 'this' 'friday' | 'this' 'coming' 'saturday' | 'this' 'coming' 'sunday' | 'this' 'coming' 'monday' | 'this' 'coming' 'tuesday' | 'this' 'coming' 'wednesday' | 'this' 'coming' 'thursday' | 'this' 'coming' 'friday' | 'day' | 'week' | 'today' | 'tomorrow' | 'the' 'day' 'after' 'tomorrow' | 'a' 'weekday' | 'weekdays'\n", "\n", "DATE -> MONTH DAY YEAR | MONTH DAY | THE DAY | MONTH DAY OR DAY | THE DAY OR DAY | EITHER MONTH DAY OR DAY | EITHER THE DAY OR THE DAY | THE DAY OF MONTH | DAY OF MONTH | EITHER THE DAY OR THE DAY OF MONTH | THE DAY OF MONTH OR THE DAY OF MONTH | EITHER THE DAY OF MONTH OR THE DAY OF MONTH\n", "MONTH -> 'january' | 'february' | 'march' | 'april' | 'may' | 'june' | 'july' | 'august' | 'september' | 'october' | 'november' | 'december'\n", "DAY -> 'first' | 'second' | 'third' | 'fourth' | 'fifth' | 'sixth' | 'seventh' | 'eighth' | 'ninth' | 'tenth' | 'eleventh' | 'twelfth' | 'thirteenth' | 'fourteenth' | 'fifteenth' | 'sixteenth' | 'seventeenth' | 'eighteenth' | 'nineteenth' | 'twentieth' | 'twenty-first' | 'twenty' 'first' | 'twenty-second' | 'twenty' 'second' | 'twenty-third' | 'twenty' 'third' | 'twenty-fourth' | 'twenty' 'fourth' | 'twenty-fifth' | 'twenty' 'fifth' | 'twenty-sixth' | 'twenty' 'sixth' | 'twenty-seventh' | 'twenty' 'seventh' | 'twenty-eighth' | 'twenty' 'eighth' | 'twenty-ninth' | 'twenty' 'ninth' | 'thirtieth' | 'thirty-first' | 'thirty' 'first' | 'one' | 'two' | 'three' | 'four' | 'five' | 'six' | 'seven' | 'eight' | 'nine' | 'ten' | 'eleven' | 'twelve' | 'thirteen' | 'fourteen' | 'fifteen' | 'sixteen' | 'seventeen' | 'eighteen' | 'nineteen' | 'twenty' | 'twenty' 'one' | 'twenty' 'two' | 'twenty' 'three' | 'twenty' 'four' | 'twenty' 'five' | 'twenty' 'six' | 'twenty' 'seven' | 'twenty' 'eight' | 'twenty' 'nine' | 'thirty' | 'thirty' 'one'\n", "YEAR -> '1991' | '1992'\n", "\n", "PAIRLINE -> 'on' | 'using' | 'of' | 'with'\n", "\n", "AIRLINE -> 'continental' | 'continental' 'airline' | 'continental' 'airlines' | 'american' 'airlines' | 'american' 'airlines' | 'american' | 'united' 'airlines' | 'united' | 'united' 'airline' | 'northwest' 'and' 'united' | 'northwest' 'airlines' | 'northwest' 'airline' | 'northwest' | 'us' 'air' | 'us' 'airlines' | 'delta' | 'twa' | 'air' 'canada' | 'eastern' 'airlines' | 'midwest' 'express' | 'trans' 'world' 'airline'\n", "\n", "AIRCRAFT -> 'using' 'a' 'j31' 'aircraft'\n", "\n", "FLIGHTTYPE -> 'round' 'trip' | 'round-trip' | 'nonstop' | 'one' 'way' | 'return' | 'direct' | 'connecting' | 'direct' 'and' 'connecting' | 'nonstop' 'or' 'connecting'\n", "\n", "FARETYPE -> 'with' 'economy' 'fares' | 'qualify' 'for' 'fare' 'code' 'qx' | 'with' 'first' 'class' 'service' | 'that' 'offer' 'first' 'class' | 'on' 'first' 'class' | 'economy' 'class' | 'with' 'the' 'lowest' 'one' 'way' 'fares' | 'with' 'a' 'class' 'of' 'service' 'code' 'f' | 'with' 'q' 'fares' | 'first' 'class' 'fare'\n", "\n", "PRICE -> 'less' 'than' '1100' 'dollars' | 'with' 'the' 'highest' 'fare' | 'the' 'cheapest' 'way' 'possible' | 'less' 'than' '466' 'dollars' | 'lowest' 'cost' | 'least' 'expensive' | 'most' 'expensive' | 'cheapest'\n", "\n", "FOOD -> 'dinner' | 'a' 'meal' | 'lunch' | 'breakfast'\n", "\n", "AVAIL -> 'available' | 'i' 'can' 'get' \n", "\n", "POSTJ -> 'please' | 'there' | 'are' | 'currently' | 'do' | 'you' | 'have' | 'fares' | 'information' | 'i' | 'want' | 'would' | 'like' | 'the' | 'flight' | 'be' | 'go' | 'departures' | 'is' | 'such' | 'a' | 'that' | 'serves' | 'both' | 'and' | 'along' | 'with' | 'can' | 'get' | \"i'd\" | 'traveling' | 'for' | 'me' | UNK | '.' | '?'\n", "\"\"\")"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "f68iSdbb3kNb"}, "source": ["To iterate over the constructed grammar, we can use `grammar.productions` method."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "A0a1eT_f3r_0"}, "outputs": [], "source": ["for production in miniATIS.productions():\n", "  lhs = production.lhs() # left hand side\n", "  rhs = production.rhs() # right hand side\n", "  print (f'Rule: {lhs} -> {rhs}')\n", "  print (f'{len(rhs)}')\n", "  break"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "YbY3oWQP2H-o"}, "source": ["To convert the constructed grammar to CNF, we use `get_cnf_grammar`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "p0CuaznJ2D5m"}, "outputs": [], "source": ["normal_miniATIS, miniATIS_wunaries = get_cnf_grammar(miniATIS)\n", "print (normal_miniATIS.is_chomsky_normal_form())"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "fMUE1HKg6DYz"}, "source": ["Now, we can use `normal_miniATIS` for CKY recognition and parsing."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "hjdpxTs-6ADC"}, "outputs": [], "source": ["for production in normal_miniATIS.productions():\n", "  lhs = production.lhs() # left hand side\n", "  rhs = production.rhs() # right hand side\n", "  print (f'Rule: {lhs} -> {rhs}')\n", "  print (f'{len(rhs)}')\n", "  break"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "Ko5PzzMkFDDt"}, "source": ["## Deterministic parsing"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "zdrkkiSAFDDt"}, "source": ["### Goal 2: CKY Recognition\n", "\n", "Implement a recognizer using the CKY algorithm to determine if a sentence is parsable. See J&M Chapter 13 for notes on CKY and the pseudo-code of the algorithm. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "uEn4JqAgGpMy"}, "outputs": [], "source": ["#TODO\n", "def cky_recognize(grammar, s):\n", "  \"\"\"\n", "  CKY algorithm for recognition.\n", "  Arguments:\n", "        grammar: a CFG in CNF\n", "        s: the input string to parse\n", "  Returns whether sentence is parsable\n", "  \"\"\"\n", "  assert(grammar.is_chomsky_normal_form())\n", "\n", "  # TODO: Implement a CKY Recognizer\n", "  return False"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "VR7poaBiGqVi"}, "source": ["#### Solution<!--Solution-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "sHkvnLV7FDDu"}, "outputs": [], "source": ["#Solution\n", "def cky_recognize(grammar, s):\n", "  \"\"\"\n", "  CKY algorithm for recognition.\n", "  Arguments:\n", "        grammar: a CFG in CNF\n", "        s: the input string to parse\n", "  Returns whether sentence is parsable\n", "  \"\"\"\n", "  assert(grammar.is_chomsky_normal_form())\n", "  nonterminals = set([])\n", "  \n", "  # Get the nonterminals\n", "  for production in grammar.productions():\n", "    nonterminals.add(production.lhs())\n", "    assert(type(production.lhs()) == Nonterminal)\n", "  num_nonterminals = len(nonterminals)\n", "  \n", "  nonterminals = list(nonterminals)\n", "  \n", "  # Setup lookup table\n", "  dynamic_table = {}\n", "\n", "  # Lookup table initialization\n", "  for i in range(len(s)):\n", "    dynamic_table[i] = {}\n", "    for j in range(len(s), i, -1):\n", "      dynamic_table[i][j] = {}\n", "      for k in nonterminals:\n", "        dynamic_table[i][j][k] = False\n", "  \n", "  # Iterate over each token\n", "  for j in range(1, len(s)+1):\n", "\n", "    # Unary productions of the form A -> token[j-1]\n", "    for production in list(filter(lambda p : len(p.rhs()) == 1 and p.rhs()[0] == s[j-1], grammar.productions())):\n", "      dynamic_table[j-1][j][production.lhs()] = True\n", "    \n", "    # Spans from size j-2 to 0\n", "    for i in range(j-2, -1, -1):\n", "      # Iterate over possible midpoints\n", "      for k in range(i+1, j):\n", "        for production in list(filter(lambda p : len(p.rhs()) == 2,grammar.productions())):\n", "          if dynamic_table[i][k][production.rhs()[0]] and dynamic_table[k][j][production.rhs()[1]]:\n", "            dynamic_table[i][j][production.lhs()] = True\n", "\n", "  # Is the sentence in the grammar?\n", "  return dynamic_table[0][len(s)][grammar.start()]"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "GKaYiJYUFlKe"}, "source": ["You can use the following test sentences to test your code:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "sryStUdRG8uq"}, "outputs": [], "source": ["# A grammartical sentence\n", "test_sentence = \"show me the flights on united that arrive before 4 between atlanta and new york\".split()\n", "result = cky_recognize(normal_miniATIS, test_sentence)\n", "print (result)\n", "\n", "# An ungrammatical sentence\n", "test_sentence = \"show me the flights on united that between atlanta and new york arrive before 4 \".split()\n", "result = cky_recognize(normal_miniATIS, test_sentence)\n", "print (result)"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "G8ggOqqbFDDw"}, "source": ["### Goal 3: CKY Parsing\n", "\n", "Implement the CKY algorithm for parsing under CFGs. You only need to add a few lines of code to your CKY recognizer to achieve this, using back-pointers. The expected return value is an NLTK tree, which can be constructed using [`Tree.fromstring`](https://www.nltk.org/_modules/nltk/tree.html).\n", "\n", "A standard tree string will be of the following form:\n", "\n", "```\n", "(S (A B) (C (D E) (F G)))\n", "```\n", "\n", "which corresponds to the following tree (drawn using tree.pretty_print()):\n", "```\n", "     S         \n", "  ___|___       \n", " |       C     \n", " |    ___|___   \n", " A   D       F \n", " |   |       |  \n", " B   E       G \n", "```"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "EezQEa2YK0d5"}, "outputs": [], "source": ["#TODO\n", "def cky_parse(grammar, s):\n", "  \"\"\"\n", "  CKY algorithm for parsing.\n", "  Arguments:\n", "        grammar: a CFG in CNF\n", "        s: the input string to parse\n", "  Returns an nltk Tree if parsable or None if not parsable\n", "  \"\"\"\n", "  assert(grammar.is_chomsky_normal_form())\n", "  \n", "  # TODO: Implement a CKY Parser\n", "  return Tree.fromstring('(S (A B) (C (D E) (F G)))')"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "QlQL-FJhLCVj"}, "source": ["#### Solution<!--Solution-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "sqLD6i1CFDDy"}, "outputs": [], "source": ["#Solution\n", "def cky_parse(grammar, s):\n", "  \"\"\"\n", "  CKY algorithm for parsing.\n", "  Arguments:\n", "        grammar: a CFG in CNF\n", "        s: the input string to parse\n", "  Returns an nltk Tree if parsable or None if not parsable\n", "  \"\"\"\n", "  assert(grammar.is_chomsky_normal_form())\n", "  nonterminals = set([])\n", "  # Get the nonterminals\n", "  for production in grammar.productions():\n", "    nonterminals.add(production.lhs())\n", "    assert(type(production.lhs()) == Nonterminal)\n", "  num_nonterminals = len(nonterminals)\n", "  \n", "  nonterminals = list(nonterminals)\n", "  \n", "  # Setup lookup table\n", "  dynamic_table = {}\n", "  \n", "  # bottom up parse tree\n", "  tree = {}\n", "\n", "  # Initialization of lookup table\n", "  for i in range(len(s)):\n", "    dynamic_table[i] = {}\n", "    tree[i] = {}\n", "    for j in range(len(s), i, -1):\n", "      dynamic_table[i][j] = {}\n", "      tree[i][j] = {}\n", "      for k in nonterminals:\n", "        dynamic_table[i][j][k] = False\n", "        tree[i][j][k] = ''\n", "        \n", "  # For each token\n", "  for j in range(1, len(s)+1):\n", "    # Unit productions of the form A -> token[j - 1]\n", "    for production in list(filter(lambda p : len(p.rhs()) == 1 and p.rhs()[0] == s[j-1], grammar.productions())):\n", "      dynamic_table[j-1][j][production.lhs()] = True\n", "      tree[j-1][j][production.lhs()] = '(' + production.lhs().__str__() + ' ' + s[j-1] + ')'\n", "\n", "    # For spans from size j-2 to 0\n", "    for i in range(j-2, -1, -1):\n", "\n", "      # Iterate over possible midpoints\n", "      for k in range(i+1, j):\n", "        # Binary productions\n", "        for production in list(filter(lambda p : len(p.rhs()) == 2,grammar.productions())):\n", "          if dynamic_table[i][k][production.rhs()[0]] and dynamic_table[k][j][production.rhs()[1]]:\n", "            dynamic_table[i][j][production.lhs()] = True\n", "            tree[i][j][production.lhs()] = '(' + production.lhs().__str__() + ' ' + tree[i][k][production.rhs()[0]] + ' ' + tree[k][j][production.rhs()[1]] + ')'\n", "\n", "  # If the sentence is in the grammar return the constructed tree\n", "  if dynamic_table[0][len(s)][grammar.start()]:\n", "    print (tree[0][len(s)][grammar.start()])\n", "    return Tree.fromstring(tree[0][len(s)][grammar.start()])\n", "  else:\n", "    return None"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "JT_CRbISLPsJ"}, "source": ["You can use the following test sentence to test your code:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "n7lbJQVGLThU"}, "outputs": [], "source": ["test_sentence = \"show me the flights on united that arrive before 4 between atlanta and new york\".split()\n", "tree = cky_parse(normal_miniATIS, test_sentence)\n", "un_cnf(tree, miniATIS_wunaries) # convert back to original grammar\n", "tree.pretty_print()"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "7VQVVkBoMITY"}, "source": ["You can also compare against a built-in nltk parser:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "BvSxIFBKMPMa"}, "outputs": [], "source": ["parser = nltk.parse.BottomUpChartParser(miniATIS)\n", "parses = [p for p in parser.parse(test_sentence)]\n", "print('Reference parse:')\n", "for ptree in parses:\n", "  print(ptree)\n", "\n", "print ('Predicted parse:')\n", "print (tree)\n", "\n", "print(\"Parses match (at least one):\", tree in parses)"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "oob8FgHQNXtx"}, "source": ["#### Grammar Coverage\n", "\n", "To check your implementation, let's compute the fraction of the original ATIS dataset that parses under the ATIS grammar. **This grammar should cover 2116 out of the 4379 sentences.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "kd_BRwjJOFTz"}, "outputs": [], "source": ["# This script might take up to 30min.\n", "# We recommend to make sure the above checks work first.\n", "all_sents_big_ATIS = []\n", "with open('data/train.nl') as f:\n", "  for line in f:\n", "    all_sents_big_ATIS.append(line.split())\n", "    \n", "parsed = 0\n", "total = 0\n", "for sent in all_sents_big_ATIS:\n", "  total += 1\n", "  sent = [tok.lower() for tok in sent]\n", "  \n", "  tree = cky_parse(normal_miniATIS, sent)\n", "  if tree:\n", "    parsed += 1\n", "  \n", "print(parsed, total)"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "MTBOhjKhFDD5"}, "source": ["## Probabilistic parsing\n", "\n", "In practice, we want to work with grammars that cover nearly all the language we expect to come across for a given application. This leads to an explosion of rules and a large number of possible parses for any one sentence. To remove ambiguity between the different parses, it's desirable to move to PCFGs. In this part, you will construct a PCFG from data, parse using a probabalistic version of CKY, and evaluate the quality of the resulting parses against gold trees."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "2fzMMPiUFDD7"}, "source": ["### Goal 4: PCFG construction\n", "\n", "Compared to CFGs, PCFGs need to assign probabilities to grammar rules. Fill in the missing pieces of the below method to build a PCFG in CNF by counting using the *train* split (`train.trees` that we downloaded in Setup). Note that we want the PCFG to be in CNF format because the probabalistic version of CKY also requires the grammar to be in CNF. However, the gold trees are not in CNF form, so in this case you will need to convert the gold *trees* to CNF before building the PCFG. To accomplish this, we will use the `treetransforms` package from `nltk`, which includes functions for converting to and from CNF.\n", "\n", "**Finish the below code to construct a PCFG grammar**. You will use it later for parsing."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "U8LP7a8LUhrV"}, "outputs": [], "source": ["#TODO\n", "def construct_pcfg_rules(gold_trees):\n", "  \"\"\"\n", "  Construct PCFG grammar rules from gold parse trees.\n", "  Arguments:\n", "        gold_trees: a list of gold parse trees\n", "  Returns a set of ProbabilisticProduction objects constructed by\n", "        ProbabilisticProduction(lhs, rhs, prob=prob)\n", "  \"\"\"\n", "  productions_with_probs = set([])\n", "\n", "  pcfg_data = defaultdict(Counter)\n", "  for tree in gold_trees:\n", "    # Convert to CNF\n", "    treetransforms.collapse_unary(tree, collapsePOS=True)\n", "    treetransforms.chomsky_normal_form(tree)\n", "    # TODO: implement here\n", "    # Construct a PCFG by counting rule frequencies from these trees\n", "    # Hint: You can get the list of productions from the tree via tree.productions()\n", "\n", "  # TODO: implement here\n", "  # Compute probabilities using the counts\n", "  return productions_with_probs"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "m9bXoIapU-Yt"}, "source": ["#### Solution<!--Solution-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "1LjD1QElFDD7"}, "outputs": [], "source": ["#Solution\n", "def construct_pcfg_rules(gold_trees):\n", "  \"\"\"\n", "  Construct PCFG grammar rules from gold parse trees.\n", "  Arguments:\n", "        gold_trees: a list of gold parse trees\n", "  Returns a set of ProbabilisticProduction objects constructed by\n", "        ProbabilisticProduction(lhs, rhs, prob=prob)\n", "  \"\"\"\n", "  productions_with_probs = set([])\n", "\n", "  pcfg_data = defaultdict(Counter)\n", "  for tree in gold_trees:\n", "    # Convert to CNF\n", "    treetransforms.collapse_unary(tree, collapsePOS=True)\n", "    treetransforms.chomsky_normal_form(tree)\n", "    # Count rule frequencies\n", "    for production in tree.productions():\n", "      pcfg_data[production.lhs()][production.rhs()] += 1\n", "  # Compute probabilities\n", "  for lhs, rhs_counter in pcfg_data.items():\n", "    total_count = sum(rhs_counter.values())\n", "    for rhs, count in rhs_counter.items():\n", "      prob = count/total_count\n", "      productions_with_probs.add(ProbabilisticProduction(lhs, rhs, prob=prob))\n", "  return productions_with_probs"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "AoPNj5xXk934"}, "outputs": [], "source": ["# Load gold trees\n", "gold_trees = []\n", "with open('data/train.trees') as f:\n", "    for line in f:\n", "      gold_trees.append(nltk.Tree.fromstring(line.strip()))\n", "      \n", "productions_with_probs = construct_pcfg_rules(gold_trees)\n", "pgrammar = PCFG(Nonterminal('TOP'), productions_with_probs)\n", "print (pgrammar.is_chomsky_normal_form())"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "ErGCrNo2FDD9"}, "source": ["### Goal 5: Probabalistic CKY parsing\n", "\n", "Finally, we are ready to implement CKY parsing under PCFGs. Adapt the CKY parser from Goal 3 to return the most likely parse and its probability given a PCFG."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "1lGbGYxeVg43"}, "outputs": [], "source": ["#TODO\n", "def cky_parse_probabalistic(grammar, s):\n", "  \"\"\"\n", "  CKY algorithm for parsing using PCFG.\n", "  Arguments:\n", "        grammar: a PCFG in CNF\n", "        s: the input string to parse\n", "  Returns twov values:\n", "        first: the most likely nltk Tree if parsable or None if not parsable\n", "        second: the highest probability\n", "  \"\"\"\n", "  assert(grammar.is_chomsky_normal_form())\n", "\n", "  # TODO: Implement a probabilisitc CKY parser\n", "  return Tree.fromstring('(S (A B) (C (D E) (F G)))'), 0"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "Ub6n6AfNWEtb"}, "source": ["#### Solution<!--Solution-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "5DotiBeBFDD9"}, "outputs": [], "source": ["#Solution\n", "def cky_parse_probabalistic(grammar, s):\n", "  \"\"\"\n", "  CKY algorithm for parsing using PCFG.\n", "  Arguments:\n", "        grammar: a PCFG in CNF\n", "        s: the input string to parse\n", "  Returns twov values:\n", "        first: the most likely nltk Tree if parsable or None if not parsable\n", "        second: the highest probability\n", "  \"\"\"\n", "  assert(grammar.is_chomsky_normal_form())\n", "  nonterminals = set([])\n", "  \n", "  # Get the nonterminals\n", "  for production in grammar.productions():\n", "    nonterminals.add(production.lhs())\n", "    assert(type(production.lhs()) == Nonterminal)\n", "  num_nonterminals = len(nonterminals)\n", "  \n", "  nonterminals = list(nonterminals)\n", "  \n", "  # Setup lookup table\n", "  dynamic_table = {}\n", "  \n", "  # bottom up parse tree\n", "  tree = {}\n", "\n", "  # Dynamic table initialization\n", "  for i in range(len(s)):\n", "    dynamic_table[i] = {}\n", "    tree[i] = {}\n", "    for j in range(len(s), i, -1):\n", "      dynamic_table[i][j] = {}\n", "      tree[i][j] = {}\n", "      for k in nonterminals:\n", "        dynamic_table[i][j][k] = None\n", "        tree[i][j][k] = ''\n", "\n", "  # Iterate over each token\n", "  for j in range(1, len(s)+1):\n", "    # Unit productions of the form A -> tokens[j-1]\n", "    for production in list(filter(lambda p : len(p.rhs()) == 1 and p.rhs()[0] == s[j-1], grammar.productions())):\n", "      dynamic_table[j-1][j][production.lhs()] = production.logprob()\n", "      tree[j-1][j][production.lhs()] = '(' + production.lhs().__str__() + ' ' + s[j-1] + ')'\n", "    \n", "    # Span length from j-2 to 0\n", "    for i in range(j-2, -1, -1):\n", "\n", "      # Iterate over all midpoints\n", "      for k in range(i+1, j):\n", "\n", "        # Iterate over each binary production\n", "        for production in list(filter(lambda p : len(p.rhs()) == 2,grammar.productions())):\n", "          A = dynamic_table[i][j][production.lhs()]\n", "          B = dynamic_table[i][k][production.rhs()[0]]\n", "          C = dynamic_table[k][j][production.rhs()[1]]\n", "          if B is not None and C is not None and (A is None or B + C + production.logprob() > A):\n", "            dynamic_table[i][j][production.lhs()] = B + C + production.logprob()\n", "            tree[i][j][production.lhs()] = '(' + production.lhs().__str__() + ' ' + tree[i][k][production.rhs()[0]] + ' ' + tree[k][j][production.rhs()[1]] + ')'\n", "\n", "  # If the sentence is in the grammar return the constructed tree\n", "  if dynamic_table[0][len(s)][grammar.start()] is not None:\n", "    return Tree.fromstring(tree[0][len(s)][grammar.start()]), 2**dynamic_table[0][len(s)][grammar.start()]\n", "  else:\n", "    return None, None"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "57Hn9aPNFDD_"}, "source": ["#### Evaluation\n", "\n", "There are a number of ways to evaluate parsing algorithms. In this assignment we will use the bracket scoring program `evalb` which we have downloaded during setup. **You are expected to achieve precision of\t0.67, recall of 0.41 and F1 of 0.51.**"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "IwEPKdvUEbbX"}, "source": ["Read in the test data:<!--TODO-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "mkyYdRYtFDED"}, "outputs": [], "source": ["all_sents = []\n", "\n", "with open('data/test.trees') as f:\n", "  for line in f:\n", "    t = Tree.fromstring(line.strip())\n", "    all_sents.append(t.leaves())"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "AQAi73xLEiPA"}, "source": ["Parse the test sentences using your implementation, and write output to a file:<!--TODO-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "-Y9ncWsgFDEF"}, "outputs": [], "source": ["trees_out = []\n", "for sent in all_sents:\n", "  tree, _ = cky_parse_probabalistic(pgrammar, sent)\n", "  if tree is not None:\n", "    trees_out.append(tree.pformat(margin=9999999999))\n", "  else:\n", "    trees_out.append('()')\n", "  \n", "with open('outp.trees', 'w') as f:\n", "  for line in trees_out:\n", "    f.write(line + '\\n')"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "q0lqBNgXzlEK"}, "source": ["Compare the predicted trees to the ground truth trees."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "Z9kudHM8FDEI"}, "outputs": [], "source": ["!python scripts/evalb.py outp.trees data/test.trees"]}], "metadata": {"colab": {"collapsed_sections": [], "include_colab_link": true, "name": "project3_tree.ipynb", "provenance": [], "toc_visible": true}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.3"}}, "nbformat": 4, "nbformat_minor": 0}